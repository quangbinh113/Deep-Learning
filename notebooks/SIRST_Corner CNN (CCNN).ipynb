{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Notice**: Rerun is suggested to carry out on Kaggle."]},{"cell_type":"markdown","metadata":{},"source":["# **1 - Setup**"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:08:00.730039Z","iopub.status.busy":"2023-01-02T08:08:00.729165Z","iopub.status.idle":"2023-01-02T08:08:04.448792Z","shell.execute_reply":"2023-01-02T08:08:04.447482Z","shell.execute_reply.started":"2023-01-02T08:08:00.729949Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'dl-sirst'...\n","remote: Enumerating objects: 35, done.\u001b[K\n","remote: Counting objects: 100% (35/35), done.\u001b[K\n","remote: Compressing objects: 100% (29/29), done.\u001b[K\n","remote: Total 35 (delta 3), reused 31 (delta 3), pack-reused 0\u001b[K\n","Unpacking objects: 100% (35/35), 1.73 MiB | 2.29 MiB/s, done.\n"]}],"source":["!git clone -b \"main\" \"https://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@github.com/minhngt62/dl-sirst.git\"\n","%mv \"dl-sirst\" \"cv_sirst\""]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:08:06.857516Z","iopub.status.busy":"2023-01-02T08:08:06.857134Z","iopub.status.idle":"2023-01-02T08:08:41.916468Z","shell.execute_reply":"2023-01-02T08:08:41.915316Z","shell.execute_reply.started":"2023-01-02T08:08:06.857483Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pycocotools\n","  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from pycocotools) (3.5.3)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pycocotools) (1.21.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (4.33.3)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (9.1.1)\n","Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.15.0)\n","Building wheels for collected packages: pycocotools\n","  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp37-cp37m-linux_x86_64.whl size=373762 sha256=a31790318bdaefce8f48cc4295bf590716396fc01706750e7c9eb0d69a9cff39\n","  Stored in directory: /root/.cache/pip/wheels/06/f6/f9/9cc49c6de8e3cf27dfddd91bf46595a057141d4583a2adaf03\n","Successfully built pycocotools\n","Installing collected packages: pycocotools\n","Successfully installed pycocotools-2.0.6\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install pycocotools"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:08:45.447549Z","iopub.status.busy":"2023-01-02T08:08:45.446602Z","iopub.status.idle":"2023-01-02T08:08:49.253335Z","shell.execute_reply":"2023-01-02T08:08:49.252006Z","shell.execute_reply.started":"2023-01-02T08:08:45.447508Z"},"trusted":true},"outputs":[],"source":["import time\n","import gc\n","import copy\n","import cv2\n","import numpy as np\n","import matplotlib.patches as patches\n","import matplotlib.pyplot as plt\n","from typing import List, Union, Dict, Optional, Tuple, Any\n","from PIL import Image\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import torch\n","from torch import nn, Tensor\n","import torchvision.transforms as T\n","import torch.nn.functional as F\n","import torchvision.ops as O\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torch.utils.tensorboard import SummaryWriter\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision as mAP\n","import torchvision"]},{"cell_type":"markdown","metadata":{},"source":["# **2 - Dataset**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:08:51.816500Z","iopub.status.busy":"2023-01-02T08:08:51.814898Z","iopub.status.idle":"2023-01-02T08:08:51.971561Z","shell.execute_reply":"2023-01-02T08:08:51.970606Z","shell.execute_reply.started":"2023-01-02T08:08:51.816460Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(tensor([[[0.7451, 0.7451, 0.7451,  ..., 0.4118, 0.4078, 0.4039],\n","          [0.7451, 0.7451, 0.7451,  ..., 0.4118, 0.4078, 0.4039],\n","          [0.7490, 0.7490, 0.7490,  ..., 0.4118, 0.4078, 0.4039],\n","          ...,\n","          [0.6157, 0.6078, 0.6039,  ..., 0.6078, 0.6118, 0.6118],\n","          [0.6118, 0.6078, 0.6039,  ..., 0.6118, 0.6157, 0.6157],\n","          [0.6078, 0.6039, 0.6000,  ..., 0.6157, 0.6196, 0.6196]]]),\n"," {'boxes': tensor([[555.,  27., 562.,  34.]]), 'labels': tensor([1])})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# load NUDT-SIRST dataset (train)\n","from cv_sirst.datasets.datasets import NUDTSIRSTDataset\n","from cv_sirst.datasets.transforms import TransformComposer, XywhToXyxy\n","\n","target_transform = TransformComposer(transforms={\n","    \"default\": [XywhToXyxy()],\n","})\n","transform = TransformComposer()\n","\n","nudtsirst_train = NUDTSIRSTDataset('/kaggle/input/nudtsirst/annotation_train.csv', \n","                                   '/kaggle/input/nudtsirst/nudtsirst', \n","                                   transform=transform, \n","                                   target_transform=target_transform)\n","nudtsirst_train[0]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:08:54.160639Z","iopub.status.busy":"2023-01-02T08:08:54.160261Z","iopub.status.idle":"2023-01-02T08:08:54.226469Z","shell.execute_reply":"2023-01-02T08:08:54.225389Z","shell.execute_reply.started":"2023-01-02T08:08:54.160607Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(tensor([[[0.4824, 0.4824, 0.4824,  ..., 0.4980, 0.4980, 0.4980],\n","          [0.4824, 0.4824, 0.4824,  ..., 0.4980, 0.4980, 0.4980],\n","          [0.4824, 0.4824, 0.4824,  ..., 0.4980, 0.4980, 0.4980],\n","          ...,\n","          [0.3882, 0.3843, 0.3765,  ..., 0.7922, 0.7725, 0.7569],\n","          [0.3922, 0.3882, 0.3765,  ..., 0.7412, 0.7333, 0.7216],\n","          [0.4039, 0.3961, 0.3843,  ..., 0.7176, 0.7137, 0.7137]]]),\n"," {'boxes': tensor([[1194.,  689., 1199.,  694.]]), 'labels': tensor([1])})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# load NUDT-SIRST dataset (test)\n","target_transform = TransformComposer(transforms={\n","    \"default\": [XywhToXyxy()],\n","}, training=False)\n","transform = TransformComposer(training=False)\n","\n","nudtsirst_test = NUDTSIRSTDataset('/kaggle/input/nudtsirst/annotation_test.csv', \n","                                  '/kaggle/input/nudtsirst/nudtsirst', \n","                                  transform=transform, \n","                                  target_transform=target_transform)\n","nudtsirst_test.eval()\n","nudtsirst_test[0]"]},{"cell_type":"markdown","metadata":{},"source":["# **3 - Models**"]},{"cell_type":"markdown","metadata":{},"source":["## 3.1 - Corner Proposal"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:08:56.138946Z","iopub.status.busy":"2023-01-02T08:08:56.138528Z","iopub.status.idle":"2023-01-02T08:08:56.151913Z","shell.execute_reply":"2023-01-02T08:08:56.150711Z","shell.execute_reply.started":"2023-01-02T08:08:56.138911Z"},"trusted":true},"outputs":[],"source":["# build anchor generator based on corner detection algorithm\n","class AnchorGenerator(nn.Module):\n","    def __init__(\n","        self, \n","        anc_size: Optional[Tuple[int, int]] = (31, 31), #  h, w\n","        max_corners: int = 600, \n","        quality_level: int = 0.002, \n","        min_distance: int = 31,\n","        ):\n","        super().__init__()\n","        self.anc_size = anc_size\n","        \n","        self.max_corners = max_corners\n","        self.quality_level = quality_level\n","        self.min_distance = min_distance\n","    \n","    def forward(\n","        self, \n","        images: List[Tensor]\n","        ) -> Tensor:\n","        anc_bases = torch.neg(torch.ones(len(images), self.max_corners, 4)) * self.min_distance # [B, n_ancs, 4]\n","        for b, image in enumerate(images):\n","            image = image.squeeze().cpu().detach().numpy()\n","            corners = cv2.goodFeaturesToTrack(image, self.max_corners, self.quality_level, self.min_distance)\n","            corners = np.int0(corners)\n","            anc_centers = torch.from_numpy(corners).squeeze()\n","            for anc_id, (x, y) in enumerate(anc_centers):\n","                xmin = x - self.anc_size[1] // 2\n","                ymin = y - self.anc_size[0] // 2\n","                xmax = x + self.anc_size[1] // 2\n","                ymax = y + self.anc_size[0] // 2\n","                anc_boxes = torch.Tensor([xmin, ymin, xmax, ymax])\n","                anc_bases[b, anc_id, :] = O.clip_boxes_to_image(anc_boxes, size=(image.shape[0], image.shape[1]))\n","            del image\n","            del corners\n","            gc.collect()\n","        anc_bases = anc_bases.cuda() if torch.cuda.is_available() else anc_bases\n","        return anc_bases"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:08:59.604963Z","iopub.status.busy":"2023-01-02T08:08:59.604322Z","iopub.status.idle":"2023-01-02T08:08:59.668651Z","shell.execute_reply":"2023-01-02T08:08:59.667623Z","shell.execute_reply.started":"2023-01-02T08:08:59.604914Z"},"trusted":true},"outputs":[],"source":["# build corner proposal module\n","class CornerProposal(nn.Module):\n","    def __init__(\n","        self, \n","        max_corners: int = 600, \n","        min_distance: int = 31,\n","        pos_thresh: float = 0.8,\n","        neg_thresh: float = 0.2,\n","        ):\n","        super().__init__()\n","        self.max_corners = max_corners\n","        self.min_distance = min_distance\n","\n","        self.pos_thresh = pos_thresh\n","        self.neg_thresh = neg_thresh\n","    \n","    def forward(\n","        self, \n","        images: Tensor, # [B, c, h, w]\n","        anc_bases: Tensor, # [B, max_corners, 4]\n","        targets: Optional[Dict[str, Tensor]] = None\n","        ) -> Tuple[Tensor, Union[Tensor, None], Union[Tensor, None], Tensor]:\n","        B, c0, h, w = len(images), images[0].size(dim=0), images[0].size(dim=1), images[0].size(dim=2)\n","        \n","        if self.training:\n","            N = targets[\"labels\"].size(dim=1) # max number of objects per image across batch\n","            gts, cls = targets[\"boxes\"], targets[\"labels\"] # [B, N, 4], [B, N]\n","            # compute an \"ioa\" matrix per image\n","            ioas_mat = torch.zeros((B, N, self.max_corners), device=gts.device) # [B, N, max_corners]\n","            for i in range(B):\n","                gt_boxes = gts[i] # [N, 4]\n","                anc_boxes = anc_bases[i] # [max_corners, 4]\n","                ioas_mat[i, :] = self._box_ioa(gt_boxes, anc_boxes) # [N, max_corners]\n","            ioas_mat = torch.transpose(ioas_mat, 1, 2) # [B, max_corners, N]\n","            gt_most_overlap_inds = torch.argmax(ioas_mat, dim=2) # [B, max_corners]\n","            max_iou_per_gt_box, _ = ioas_mat.max(dim=1, keepdim=True) # [B, 1, N]\n","\n","            # get positive anchor boxes\n","            positive_anc_mask = torch.logical_and(ioas_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0)\n","            positive_anc_mask = torch.logical_or(positive_anc_mask, ioas_mat > self.pos_thresh) # [B, max_corners, 1]\n","            pos_inds = positive_anc_mask.nonzero(as_tuple=True)[:-1] # 2 tensors of indices\n","            # get negative anchor boxes\n","            negative_anc_mask = ioas_mat < self.neg_thresh # [B, max_corners, 1]\n","            neg_inds = negative_anc_mask.nonzero(as_tuple=True)[:-1] # 2 tensors of indices\n","\n","            if pos_inds[0].numel() != 0 or neg_inds[0].numel() != 0:\n","                # map gts to corr anchors\n","                gts_expand = gts.view(B, 1, N, 4).expand(B, self.max_corners, N, 4)\n","                ancs_to_gts = torch.gather(gts_expand, -2, gt_most_overlap_inds.reshape(B, self.max_corners, 1, 1).repeat(1, 1, 1, 4)) # [B, max_corners, 1, 4]\n","                ancs_to_gts = ancs_to_gts.flatten(start_dim=2) # [B, max_corners, 4]\n","                ancs_to_gts = torch.where(positive_anc_mask, ancs_to_gts, torch.tensor(-self.min_distance, device=gts.device).float())\n","\n","                # map cls to corr anchors\n","                cls_expand = cls.view(B, 1, N).expand(B, self.max_corners, N)\n","                ancs_to_cls = torch.gather(cls_expand, -1, gt_most_overlap_inds.unsqueeze(-1)).squeeze(-1) # [B, max_corners]\n","                ancs_to_cls = torch.where(positive_anc_mask.flatten(start_dim=1), ancs_to_cls, 0)\n","\n","                # extract a sastified anchor's indices\n","                pos_ind_selected = torch.stack(pos_inds)[:, :B] # [2, ~B]\n","                neg_ind_selected = torch.stack(neg_inds)[:, :pos_ind_selected.size(dim=1)] # [2, ~B]\n","                roi_inds = tuple(torch.cat((pos_ind_selected, neg_ind_selected), dim=1)) # [2, ~2xB]\n","\n","                # extract ROIs\n","                roi_bases = anc_bases[roi_inds] # [~2xB, 4] -> xmin, ymin, xmax, ymax\n","                roi_centers = (roi_bases[:, :2] + roi_bases[:, 2:]) // 2 # [~2xB, 2]\n","                images = images[roi_inds[:-1]] # [~2xB, c, h, w]\n","                rois = self._extract_glimpse(\n","                    images, \n","                    size=(self.min_distance, self.min_distance), \n","                    offsets=roi_centers\n","                    ) # [~2xB, c, min_distance, min_distance]\n","                roi_cls = ancs_to_cls[roi_inds] # [~2xB]\n","                #roi_gts = (ancs_to_gts[roi_inds] - torch.cat((roi_bases[roi_inds[:-1]][:, :2], roi_bases[roi_inds[:-1]][:, :2]), dim=1)) / (self.min_distance - 1) # [~2xB, 4]\n","                roi_gts = ancs_to_gts[roi_inds]\n","\n","                return rois, roi_cls, roi_gts, roi_bases[roi_inds[:-1]][:, :2] # [~2xB, 2]\n","        \n","        anc_centers = (anc_bases[:, :, :2] + anc_bases[:, :, :2]) // 2 # [B, max_corners, 2]\n","        rois = self._extract_glimpses(\n","            images, \n","            size=(self.min_distance, self.min_distance), \n","            offsets=anc_centers\n","            ) # [B, max_corners, c, min_distance, min_distance]\n","        return rois, None, None, anc_bases[:, :, :2] # [B, max_corners, 2]\n","\n","    def _box_ioa(\n","        self, \n","        gt_boxes: Tensor, \n","        anc_boxes: Tensor\n","        ) -> Tensor:\n","        ioa_mat = torch.zeros((gt_boxes.size(dim=0), anc_boxes.size(dim=0)))\n","        for i in range(len(gt_boxes)):\n","            xmin = torch.max(gt_boxes[i, 0], anc_boxes[:, 0])\n","            ymin = torch.max(gt_boxes[i, 1], anc_boxes[:, 1])\n","            xmax = torch.min(gt_boxes[i, 2], anc_boxes[:, 2])\n","            ymax = torch.min(gt_boxes[i, 3], anc_boxes[:, 3])\n","\n","            w = (xmax - xmin + 1).double()\n","            h = (ymax - ymin + 1).double()\n","            intersection = torch.where((w > 0) & (h > 0), w * h, 0.)\n","            gt_area = (gt_boxes[i, 2] - gt_boxes[i, 0] + 1) * (gt_boxes[i, 3] - gt_boxes[i, 1] + 1)\n","            ioa_mat[i, :] = intersection / gt_area\n","        return ioa_mat\n","\n","    def _extract_glimpse(\n","        self,\n","        input: Tensor, # [B, C, H, W]\n","        size: Tuple[int, int],\n","        offsets: Tensor, # [B, 2]\n","        centered=False, \n","        normalized=False, \n","        mode='bilinear', \n","        padding_mode='zeros'\n","        ) -> Tensor:\n","        W, H = input.size(-1), input.size(-2)\n","\n","        if normalized and centered:\n","            offsets = (offsets + 1) * offsets.new_tensor([W/2, H/2])\n","        elif normalized:\n","            offsets = offsets * offsets.new_tensor([W, H])\n","        elif centered:\n","            raise ValueError(\n","                f'Invalid parameter that offsets centered but not normlized')\n","\n","        h, w = size\n","        xs = torch.arange(0, w, dtype=input.dtype,\n","                        device=input.device) - (w - 1) / 2.0\n","        ys = torch.arange(0, h, dtype=input.dtype,\n","                        device=input.device) - (h - 1) / 2.0\n","\n","        vy, vx = torch.meshgrid(ys, xs)\n","        grid = torch.stack([vx, vy], dim=-1)  # h, w, 2\n","\n","        offsets_grid = offsets[:, None, None, :] + grid[None, ...]\n","\n","        # normalised grid to [-1, 1]\n","        offsets_grid = (\n","            offsets_grid - offsets_grid.new_tensor([W/2, H/2])) / offsets_grid.new_tensor([W/2, H/2])\n","\n","        return torch.nn.functional.grid_sample(\n","            input, offsets_grid, mode=mode, align_corners=False, padding_mode=padding_mode)\n","\n","    def _extract_glimpses(\n","        self,\n","        input: Tensor, # [B, C, H, W]\n","        size: Tuple[int, int],\n","        offsets: Tensor, # [B, max_corners, 2]\n","        centered=False, \n","        normalized=False, \n","        mode='bilinear', \n","        padding_mode='zeros'\n","        ) -> Tensor:\n","        patches = [] # [max_corners, B, c, size, size]\n","        for i in range(offsets.size(-2)):\n","            patch = self._extract_glimpse(input, size, offsets[:, i, :], centered, normalized, mode) # [B, c, size, size]\n","            patches.append(patch)\n","        return torch.stack(patches, dim=1) # [B, max_corners, c, size, size]"]},{"cell_type":"markdown","metadata":{},"source":["## 3.2 - Convolutional Neural Block "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:09:04.267748Z","iopub.status.busy":"2023-01-02T08:09:04.267358Z","iopub.status.idle":"2023-01-02T08:09:04.275374Z","shell.execute_reply":"2023-01-02T08:09:04.273760Z","shell.execute_reply.started":"2023-01-02T08:09:04.267696Z"},"trusted":true},"outputs":[],"source":["# build the MLP head\n","class TwoMLPHead(nn.Module):\n","    def __init__(self, in_features=512):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.loc_fc = nn.Linear(in_features, 4)\n","        self.cls_fc = nn.Linear(in_features, 1)\n","        self.sigmoid = nn.Sigmoid()\n","    \n","    def forward(self, x):\n","        x = self.flatten(x)\n","        box = self.sigmoid(self.loc_fc(x))\n","        score = self.sigmoid(self.cls_fc(x))\n","        return box, score"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:09:07.366611Z","iopub.status.busy":"2023-01-02T08:09:07.366234Z","iopub.status.idle":"2023-01-02T08:09:12.965710Z","shell.execute_reply":"2023-01-02T08:09:12.964758Z","shell.execute_reply.started":"2023-01-02T08:09:07.366578Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81e15032e3eb453a93b2bb1038212212","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/83.3M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# setup resnet-34\n","resnet = torchvision.models.resnet34(pretrained=True)\n","resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","resnet.fc = TwoMLPHead()\n","\n","# transfer learning\n","for param in resnet.parameters():\n","    param.requires_grad = False\n","resnet.conv1.weight.requires_grad = True\n","for name, weight in resnet.fc.named_parameters():\n","    weight.requires_grad = True"]},{"cell_type":"code","execution_count":10,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-01-02T08:09:24.232411Z","iopub.status.busy":"2023-01-02T08:09:24.231710Z","iopub.status.idle":"2023-01-02T08:09:24.243253Z","shell.execute_reply":"2023-01-02T08:09:24.241597Z","shell.execute_reply.started":"2023-01-02T08:09:24.232373Z"},"trusted":true},"outputs":[],"source":["# build a light-weight CNN --DEPRECATED\n","class LightWeightCNN(nn.Module):\n","    def __init__(self, in_channels, out_channels=4):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        \n","        conv_block = []\n","        channels = [in_channels, 8, 16, 32]\n","        for i in range(1, len(channels)):\n","            conv = nn.Sequential(*[\n","                nn.Conv2d(in_channels=channels[i-1], out_channels=channels[i], kernel_size=3, stride=1, padding=\"same\", bias=False),\n","                nn.BatchNorm2d(num_features=channels[i]),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(in_channels=channels[i], out_channels=channels[i], kernel_size=3, stride=1, padding=\"same\", bias=False),\n","                nn.BatchNorm2d(num_features=channels[i]),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n","            ])\n","            conv_block += list(conv)\n","        self.conv_block = nn.Sequential(*conv_block)\n","        self.bottleneck = O.misc.ConvNormActivation(in_channels=channels[-1], out_channels=out_channels, kernel_size=1, stride=1, padding=\"same\")\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n","        self.mlp_head = TwoMLPHead(out_channels)\n","        \n","    def forward(self, x):\n","        x = self.conv_block(x)\n","        x = self.bottleneck(x)\n","        x = self.global_avg_pool(x)\n","        x = self.mlp_head(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## 3.3 - Main Model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:09:34.921205Z","iopub.status.busy":"2023-01-02T08:09:34.920770Z","iopub.status.idle":"2023-01-02T08:09:34.939219Z","shell.execute_reply":"2023-01-02T08:09:34.938314Z","shell.execute_reply.started":"2023-01-02T08:09:34.921171Z"},"trusted":true},"outputs":[],"source":["# build loss function DIoU\n","class DIoULoss(nn.Module):\n","    \"\"\"\n","    Distance Intersection over Union Loss (Zhaohui Zheng et. al)\n","    https://arxiv.org/abs/1911.08287\n","    Args:\n","        input, target (Tensor): box locations in XYXY format, shape (N, 4) or (4,).\n","        reduction: 'none' | 'mean' | 'sum'\n","                 'none': No reduction will be applied to the output.\n","                 'mean': The output will be averaged.\n","                 'sum': The output will be summed.\n","        eps (float): small number to prevent division by zero\n","    \"\"\"\n","    __constant__ = [\"none\", \"sum\", \"mean\"]\n","    \n","    def __init__(\n","        self,\n","        eps: float = 1e-7,\n","        reduction: Optional[str] = None,\n","        weights: Optional[Tensor] = None\n","        ):\n","        super(DIoULoss, self).__init__()\n","        self.eps = eps\n","        self.reduction = reduction\n","        self.weights = weights\n","    \n","    def forward(\n","        self,\n","        input: Tensor,\n","        target: Tensor\n","        ) -> Tensor:\n","        intsct, union = self._loss_inter_union(input, target)\n","        iou = intsct / (union + self.eps)\n","        \n","        # smallest enclosing box\n","        x1, y1, x2, y2 = input.unbind(dim=-1)\n","        x1g, y1g, x2g, y2g = target.unbind(dim=-1)\n","        xc1 = torch.min(x1, x1g)\n","        yc1 = torch.min(y1, y1g)\n","        xc2 = torch.max(x2, x2g)\n","        yc2 = torch.max(y2, y2g)\n","        \n","        # the diagonal distance of the smallest enclosing box squared\n","        diagonal_distance_squared = ((xc2 - xc1) ** 2) + ((yc2 - yc1) ** 2) + self.eps\n","        \n","        # centers of boxes\n","        x_p = (x2 + x1) / 2\n","        y_p = (y2 + y1) / 2\n","        x_g = (x1g + x2g) / 2\n","        y_g = (y1g + y2g) / 2\n","        \n","        # the distance between boxes' centers squared.\n","        centers_distance_squared = ((x_p - x_g) ** 2) + ((y_p - y_g) ** 2)\n","        \n","        # distance between boxes' centers squared.\n","        loss = 1 - iou + (centers_distance_squared / diagonal_distance_squared)\n","        \n","        # eqn. (7)\n","        loss = 1 - iou + (centers_distance_squared / diagonal_distance_squared)\n","        if self.weights is not None:\n","            loss = loss * self.weights\n","        loss = loss[torch.nonzero(loss, as_tuple=True)]\n","        \n","        if self.reduction == \"mean\":\n","            loss = loss.mean() if loss.numel() > 0 else 0.0 * loss.sum()\n","        elif self.reduction == \"sum\":\n","            loss = loss.sum()\n","        return loss\n","    \n","    def _loss_inter_union(\n","        self,\n","        boxes1: torch.Tensor,\n","        boxes2: torch.Tensor\n","        ) -> Tuple[torch.Tensor, torch.Tensor]:\n","\n","        x1, y1, x2, y2 = boxes1.unbind(dim=-1)\n","        x1g, y1g, x2g, y2g = boxes2.unbind(dim=-1)\n","\n","        # Intersection keypoints\n","        xkis1 = torch.max(x1, x1g)\n","        ykis1 = torch.max(y1, y1g)\n","        xkis2 = torch.min(x2, x2g)\n","        ykis2 = torch.min(y2, y2g)\n","\n","        intsctk = torch.zeros_like(x1)\n","        mask = (ykis2 > ykis1) & (xkis2 > xkis1)\n","        intsctk[mask] = (xkis2[mask] - xkis1[mask]) * (ykis2[mask] - ykis1[mask])\n","        unionk = (x2 - x1) * (y2 - y1) + (x2g - x1g) * (y2g - y1g) - intsctk\n","\n","        return intsctk, unionk"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:09:38.400523Z","iopub.status.busy":"2023-01-02T08:09:38.400126Z","iopub.status.idle":"2023-01-02T08:09:38.425744Z","shell.execute_reply":"2023-01-02T08:09:38.424623Z","shell.execute_reply.started":"2023-01-02T08:09:38.400489Z"},"trusted":true},"outputs":[],"source":["# build the total model\n","class CCNN(nn.Module):\n","    def __init__(\n","        self,\n","        predictor: nn.Module,\n","        roi_transform: Any = None,\n","        \n","        anchor_size: Optional[Tuple[int, int]] = (31, 31),\n","        max_corners: int = 600, \n","        quality_level: int = 0.002, \n","        min_distance: int = 11,\n","        \n","        pos_thresh: float = 0.7,\n","        neg_thresh: float = 0.2,\n","        \n","        score_thresh: float = 0.5,\n","        loss_reg_cls_ratio: float = 0.80,\n","        nms_thresh: float = 0.6\n","        ):\n","        super().__init__()\n","        self.score_thresh = score_thresh\n","        self.reg_ratio = loss_reg_cls_ratio\n","        self.nms_thresh = nms_thresh\n","        \n","        self.anchor_generator = AnchorGenerator(anchor_size, max_corners, quality_level, min_distance)\n","        self.corner_proposal = CornerProposal(max_corners, min_distance, pos_thresh, neg_thresh)\n","        self.roi_transform = roi_transform\n","        self.predictor = predictor\n","    \n","    def forward(\n","        self, \n","        images: Optional[List[Tensor]], \n","        targets: Optional[List[Dict[str, Tensor]]] = None\n","        ) -> Union[Tensor, List[Dict[str, Tensor]]]:\n","        torch._assert(images is not None, \"[ERROR] Images cannot be missing\")\n","        if self.training:\n","            torch._assert(targets is not None, \"[ERROR] Targets should not be none during training\")\n","            targets = self._stack_targets(targets) # [B, N, 4], [B, N]\n","        \n","        anc_bases = self.anchor_generator(images)\n","        if images is not Tensor:\n","            images = torch.stack(images)\n","        h, w = images.size(dim=-2), images.size(dim=-1)\n","        rois, roi_cls, roi_gts, roi_uplefts = self.corner_proposal(images, anc_bases, targets)\n","        if self.roi_transform:\n","            rois = self.roi_transform(rois)\n","        \n","        if self.training:\n","            locs, scores = self.predictor(rois) # [B, out_channels, 1, 1]\n","            locs = self._decode_loc(locs, roi_uplefts) # [~2xB, 4]\n","            scores = torch.flatten(scores, start_dim=0) # [~2xB]\n","            loss = self.compute_loss(locs, scores, roi_gts, roi_cls)\n","            return loss\n","        \n","        detections = []\n","        for b, ins_rois in enumerate(rois):\n","            locs, scores = self.predictor(ins_rois)\n","            locs = self._decode_loc(locs, roi_uplefts[b]) # [~2xB, 4]\n","            scores = torch.flatten(scores, start_dim=0) # [~2xB]\n","            locs, scores, labels = self.postprocess_detections(locs, scores, (h, w))\n","            detections.append(self._one_detection(locs, scores, labels))\n","        return detections\n","            \n","    def compute_loss(self, locs, scores, gts, labels) -> Tensor:\n","        loc_loss_fn = DIoULoss(reduction=\"mean\", weights=labels)\n","        cls_loss_fn = nn.BCELoss()\n","        cls_loss = cls_loss_fn(scores, labels.float())\n","        loc_loss = loc_loss_fn(locs, gts)\n","        return (1 - self.reg_ratio) * cls_loss + self.reg_ratio * loc_loss\n","    \n","    def _stack_targets(\n","        self, \n","        targets: List[Dict[str, Tensor]]\n","        ) -> Dict[str, Tensor]:\n","        target_stack = {}\n","        for k in targets[0].keys():\n","            tensors = (targets[0][k],)\n","            for i in range(1, len(targets)):\n","                tensors = tensors + (targets[i][k],)\n","            target_stack[k] = torch.stack(tensors)\n","        return target_stack\n","        \n","    def _one_detection(self, locs, scores, labels) -> Dict[str, Tensor]:\n","        return {\n","            \"boxes\": locs, # [~2xB, 4]\n","            \"scores\": scores, # [~2xB]\n","            \"labels\": labels # [~2xB]\n","        }\n","    \n","    def _decode_loc(self, locs, roi_uplefts):\n","        locs = locs * (self.corner_proposal.min_distance - 1) + torch.cat((roi_uplefts, roi_uplefts), dim=1) # [~2xB, 4]\n","        locs[:, 2:] = torch.ceil(locs[:, 2:])\n","        locs[:, :2] = torch.floor(locs[:, :2])\n","        return locs # [~2xB, 4]\n","    \n","    def postprocess_detections(self, locs, scores, image_shape):\n","        locs = O.clip_boxes_to_image(locs, image_shape)\n","        labels = torch.where(scores.double() > self.score_thresh, 1, 0) # [~2xB]\n","\n","        # remove low scoring boxes\n","        keep = torch.where(scores > self.score_thresh)\n","        locs, labels, scores = locs[keep], labels[keep], scores[keep]\n","\n","        # remove empty boxes\n","        keep = O.remove_small_boxes(locs, min_size=1e-3)\n","        locs, scores, labels = locs[keep], scores[keep], labels[keep]\n","\n","        # non-maximum suppression\n","        keep = O.nms(locs, scores, self.nms_thresh)\n","        locs, scores, labels = locs[keep], scores[keep], labels[keep]\n","\n","        return locs, scores, labels"]},{"cell_type":"markdown","metadata":{},"source":["# **4 - Training Phase**"]},{"cell_type":"markdown","metadata":{},"source":["## 4.1 - Metrics"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:09:41.881499Z","iopub.status.busy":"2023-01-02T08:09:41.881130Z","iopub.status.idle":"2023-01-02T08:09:41.896736Z","shell.execute_reply":"2023-01-02T08:09:41.894352Z","shell.execute_reply.started":"2023-01-02T08:09:41.881466Z"},"trusted":true},"outputs":[],"source":["# build the custom metrics for \n","class SIRSTMetrics:\n","    def __init__(\n","        self, \n","        iou_thresholds: List[float] = [0.0, 0.5, 1.0],\n","        eps: float = 1e-7\n","        ):\n","        self.true_pos = [0] * len(iou_thresholds)\n","        self.false_pos = [0] * len(iou_thresholds)\n","        self.n_preds = 0\n","        self.n_gts = 0\n","        self.iou_thresholds = iou_thresholds\n","        self.eps = eps\n","    \n","    def compute(self) -> Dict[str, Tensor]:\n","        true_pos = Tensor(self.true_pos, device=self._device)\n","        false_pos = Tensor(self.false_pos, device=self._device)\n","        detect_rate = true_pos / (self.n_gts + self.eps) # true / n_targets\n","        false_alarm = false_pos / (self.n_preds + self.eps) # false / n_preds\n","        return {f\"detection_rate_{self.iou_thresholds[i]}\": detect_rate[i] for i in range(len(self.iou_thresholds))}, {f\"false_alarm_rate_{self.iou_thresholds[i]}\": false_alarm[i] for i in range(len(self.iou_thresholds))}\n","    \n","    def update(\n","        self, \n","        preds: List[Dict[str, Tensor]], # [N, ...]\n","        targets: List[Dict[str, Tensor]] # [M, ...]\n","        ):\n","        self._device = targets[0][\"boxes\"].device\n","        max_preds = len(max(preds, key=lambda r: len(r[\"boxes\"]))[\"boxes\"])\n","        max_targets = len(max(targets, key=lambda r: len(r[\"boxes\"]))[\"boxes\"])\n","        ious_mat = torch.zeros((len(preds), max_preds, max_targets), device=self._device) # [B, max_preds, max_targets]\n","        for i in range(len(preds)):\n","            pred = preds[i] # [N, 4]\n","            target = targets[i] # [M, 4]\n","            ious_mat[i, :] = O.box_iou(pred[\"boxes\"], target[\"boxes\"]) # [N, M]\n","            self.n_preds += pred[\"boxes\"].size(dim=0)\n","            self.n_gts += target[\"boxes\"].size(dim=0)\n","        for i, iou_threshold in enumerate(self.iou_thresholds):\n","            true_pos_inds = torch.where(ious_mat > iou_threshold)\n","            false_pos_inds = torch.where(ious_mat <= iou_threshold)\n","            self.true_pos[i] += len(true_pos_inds[0])\n","            self.false_pos[i] += len(false_pos_inds[0])"]},{"cell_type":"markdown","metadata":{},"source":["## 4.2 - Setup Live Tensorboard"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:15:17.272991Z","iopub.status.busy":"2023-01-02T08:15:17.271949Z","iopub.status.idle":"2023-01-02T08:15:25.051595Z","shell.execute_reply":"2023-01-02T08:15:25.050352Z","shell.execute_reply.started":"2023-01-02T08:15:17.272952Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-01-02 08:15:18--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 54.237.133.81, 18.205.222.128, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13832437 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","ngrok-stable-linux- 100%[===================>]  13.19M  3.43MB/s    in 5.0s    \n","\n","2023-01-02 08:15:23 (2.63 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n","\n","Archive:  ./ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"]}],"source":["# dowload ngrok to launch tensorboard\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ./ngrok-stable-linux-amd64.zip"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:15:25.057847Z","iopub.status.busy":"2023-01-02T08:15:25.056686Z","iopub.status.idle":"2023-01-02T08:15:26.058447Z","shell.execute_reply":"2023-01-02T08:15:26.057281Z","shell.execute_reply.started":"2023-01-02T08:15:25.057804Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"]}],"source":["# add auth-token\n","!./ngrok authtoken 2IuQZfwfPwjKtQzNIBEzatlGJ1U_5qibyn8Ef9htdqoBus2Kw"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:15:26.061625Z","iopub.status.busy":"2023-01-02T08:15:26.061204Z","iopub.status.idle":"2023-01-02T08:15:26.163208Z","shell.execute_reply":"2023-01-02T08:15:26.159965Z","shell.execute_reply.started":"2023-01-02T08:15:26.061582Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-01-02 08:15:31.475112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-01-02 08:15:31.554954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-01-02 08:15:31.555883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","\n","NOTE: Using experimental fast data loading logic. To disable, pass\n","    \"--load_fast=false\" and report issues on GitHub. More details:\n","    https://github.com/tensorflow/tensorboard/issues/4784\n","\n","TensorBoard 2.10.1 at http://0.0.0.0:6006/ (Press CTRL+C to quit)\n"]}],"source":["# launch and tunnel tensorboard\n","import os\n","import multiprocessing\n"," \n","pool = multiprocessing.Pool(processes = 10)\n","results_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n","                        for cmd in [\n","                        f\"tensorboard --logdir ./runs/ --host 0.0.0.0 --port 6006 &\",\n","                        \"./ngrok http 6006 &\"\n","                        ]]"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:15:42.732858Z","iopub.status.busy":"2023-01-02T08:15:42.732390Z","iopub.status.idle":"2023-01-02T08:15:53.818111Z","shell.execute_reply":"2023-01-02T08:15:53.816870Z","shell.execute_reply.started":"2023-01-02T08:15:42.732818Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["http://5a94-35-233-141-223.ngrok.io\n"]}],"source":["# curl ngrok port\n","import time\n","time.sleep(10) # wait for tensorboard host\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""]},{"cell_type":"markdown","metadata":{},"source":["## 4.3 - Trainer: Model Wrapper"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:15:54.778641Z","iopub.status.busy":"2023-01-02T08:15:54.778251Z","iopub.status.idle":"2023-01-02T08:15:54.826046Z","shell.execute_reply":"2023-01-02T08:15:54.825110Z","shell.execute_reply.started":"2023-01-02T08:15:54.778605Z"},"trusted":true},"outputs":[],"source":["# build trainer to wrap model\n","from cv_sirst.datasets.datasets import SIRSTDataset\n","from cv_sirst.trainers import callbacks as cb\n","\n","class Trainer:\n","    MAX_LOAD_SIZE = 8 # 15GB VRAM\n","\n","    def __init__(\n","        self,\n","        model: nn.Module,\n","        optimizer: torch.optim.Optimizer,\n","        dataset: SIRSTDataset,\n","        batch_size: int = 32,\n","        valid_ratio: int = 0.05,\n","        valid_freq: int  = 1,\n","        tensorboard: SummaryWriter = None,\n","        output: Union[str, None] = None,\n","        input: Union[str, None] = None,\n","        log_freq_gradient: int = 10,\n","        metric: Any = mAP(iou_thresholds=[0.1*i for i in range(1, 6)], rec_thresholds=list(np.linspace(0, 1, 11)), max_detection_thresholds=100),\n","        lr_scheduler=None\n","    ):\n","        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.lr_scheduler = lr_scheduler\n","        \n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.valid_ratio = valid_ratio\n","        self.valid_freq = valid_freq\n","        self.data_loader, self.data_loader_valid = self._train_test_split()\n","\n","        self.input = input\n","        self.output = output\n","        \n","        self.tensorboard = tensorboard\n","        self.save_model = cb.BestCheckpoint(output=output)\n","        self.display = cb.Displayer()\n","        self.metric = metric\n","\n","        self.log_freq_gradient = log_freq_gradient\n","\n","        self.model.to(self.device)\n","\n","    def fit(\n","        self,\n","        start_epoch: int = 0, # inclusive\n","        end_epoch: int = 20, # exclusive\n","        ):\n","        warm_up=True\n","        for epoch in range(start_epoch, end_epoch):\n","            if not warm_up:\n","                print() # vanish \\r\n","            else:\n","                warm_up=False\n","            print(f\"Epoch {epoch+1}/{end_epoch}\")\n","            self.train_one_epoch(epoch)\n","\n","    def train_one_epoch(self, epoch):\n","        for iter, (images, targets) in enumerate(self.data_loader):\n","            self.train_one_step(iter, images, targets, epoch)\n","            if self.lr_scheduler is not None:\n","                self.lr_scheduler.step()\n","            \n","    def train_one_step(self, iter, images, targets, epoch):\n","        ts = time.time()\n","        self.model.train()\n","        \n","        self.optimizer.zero_grad()\n","        chunks = [(images[i:i+self.MAX_LOAD_SIZE], targets[i:i+self.MAX_LOAD_SIZE]) for i in range(0, len(images), self.MAX_LOAD_SIZE)]\n","        for imgs, tars in chunks:\n","            imgs = list(image.to(self.device) for image in imgs)\n","            tars = [{k: v.to(self.device) for k, v in t.items()} for t in tars]\n","            \n","            losses = self.model(imgs, tars)\n","            losses = losses / len(chunks) # normalize across chunks\n","\n","            del imgs\n","            del tars\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","\n","            losses.backward()\n","        \n","        if self.log_freq_gradient != 0 and iter % self.log_freq_gradient == 0:\n","            for name, weight in self.model.named_parameters():\n","                if weight.requires_grad:\n","                    self.tensorboard.add_histogram(name + \".grad\", weight.grad, epoch * len(self.data_loader) + iter)\n","        self.optimizer.step()\n","\n","        te = time.time()\n","        valid_losses = None\n","        if iter % int(self.valid_freq * len(self.data_loader)) == 0 and iter != 0:\n","            valid_losses = self.validate()\n","            for i, valid_loss in enumerate(valid_losses):\n","                self.tensorboard.add_scalar(\"valid/metric_{i}\", valid_loss, epoch * len(self.data_loader) + iter)\n","                self.save_model.quality_save((sum(valid_loss.values()) / len(valid_loss)).item(), epoch, self.model, self.optimizer, output=f\"metric_{i}_\" + self.output)\n","        \n","        valid_loss = None\n","        if valid_losses is not None:\n","            valid_loss = valid_losses[0]\n","            valid_loss = (sum(valid_loss.values()) / len(valid_loss)).item()\n","        \n","        self.display(len(self.data_loader), iter+1, te-ts, losses.item(), valid_loss=valid_loss)\n","        self.tensorboard.add_scalar(\"train/loss\", losses.item(), epoch * len(self.data_loader) + iter)\n","        #self.tensorboard.add_scalar(\"train/lr\", self.optimizer.param_groups[0]['lr'], epoch * len(self.data_loader) + iter)\n","        self.save_model(losses.item(), epoch, self.model, self.optimizer)\n","\n","    def validate(self):\n","        self.model.eval()\n","        with torch.no_grad():\n","            for images, targets in self.data_loader_valid:\n","                chunks = [(images[i:i+self.MAX_LOAD_SIZE], targets[i:i+self.MAX_LOAD_SIZE]) for i in range(0, len(images), self.MAX_LOAD_SIZE)]\n","                for imgs, tars in chunks:\n","                    imgs = list(image.to(self.device) for image in imgs)\n","                    tars = [{k: v.to(self.device) for k, v in t.items()} for t in tars]\n","                    detections = self.model(imgs, tars)\n","                    \n","                    self.metric.update(detections, tars)\n","                    del imgs\n","                    del tars\n","                    gc.collect()\n","                    torch.cuda.empty_cache()\n","        return self.metric.compute()\n","    \n","    @torch.inference_mode()\n","    def evaluate(self, data_loader_test: DataLoader, print_freq=10) -> Dict[str, Tensor]:\n","        with torch.no_grad():\n","            for images, targets in self.data_loader_test:\n","                chunks = [(images[i:i+self.MAX_LOAD_SIZE], targets[i:i+self.MAX_LOAD_SIZE]) for i in range(0, len(images), self.MAX_LOAD_SIZE)]\n","                for imgs, tars in chunks:\n","                    imgs = list(image.to(self.device) for image in imgs)\n","                    tars = [{k: v.to(self.device) for k, v in t.items()} for t in tars]\n","                    detections = self.model(imgs, tars)\n","                    \n","                    self.metric.update(detections, tars)\n","                    del imgs\n","                    del tars\n","                    gc.collect()\n","                    torch.cuda.empty_cache()\n","        return self.metric.compute()\n","        \n","    def _train_test_split(self) -> Tuple[DataLoader]:\n","        dataset_valid = copy.deepcopy(self.dataset)\n","        dataset_valid.transform.training = False\n","        dataset_valid.target_transform.training = False\n","        \n","        torch.manual_seed(1)\n","        split_idx = int(self.valid_ratio * len(self.dataset))\n","        indices = torch.randperm(len(self.dataset)).tolist()\n","        dataset = Subset(self.dataset, indices[:-split_idx])\n","        dataset_valid = Subset(dataset_valid, indices[-split_idx:])\n","\n","        data_loader = DataLoader(\n","            dataset, batch_size=self.batch_size, \n","            shuffle=True, collate_fn=lambda batch: tuple(zip(*batch)))\n","        valid_data_loader = DataLoader(\n","            dataset_valid, batch_size=self.batch_size,\n","            shuffle=False, collate_fn=lambda batch: tuple(zip(*batch)))\n","        \n","        return data_loader, valid_data_loader\n","    \n","    def __setattr__(self, __name: str, __value: Any) -> None:\n","        if __name == \"input\":\n","            if __value is not None:\n","                try:\n","                    checkpoint = torch.load(__value, map_location=\"cpu\")\n","                    self.model.load_state_dict(checkpoint['model_state_dict'])\n","                    self.model.to(self.device)\n","                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","                    cb.optimizer_to(self.optimizer, self.device)\n","                except:\n","                    if self.model is None or self.optimizer is None:\n","                        print(\"[ERROR] - Must have the desired model/optimizer\")\n","                    print(\"[ERROR] - The path would be wrong.\")\n","\n","        super(Trainer, self).__setattr__(__name, __value)\n","        \n","        if __name in (\"dataset\", \"batch_size\", \"valid_ratio\"):\n","            if __name == \"batch_size\":\n","                assert self.batch_size % self.MAX_LOAD_SIZE == 0\n","            try:\n","                self.data_loader, self.data_loader_valid = self._train_test_split()\n","            except AttributeError:\n","                pass"]},{"cell_type":"markdown","metadata":{},"source":["## 4.4 - Training Models"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:15:59.962105Z","iopub.status.busy":"2023-01-02T08:15:59.961118Z","iopub.status.idle":"2023-01-02T08:16:00.976244Z","shell.execute_reply":"2023-01-02T08:16:00.975044Z","shell.execute_reply.started":"2023-01-02T08:15:59.962065Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Jan  2 08:16:00 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T08:18:40.658578Z","iopub.status.busy":"2023-01-02T08:18:40.658187Z","iopub.status.idle":"2023-01-02T08:18:40.887211Z","shell.execute_reply":"2023-01-02T08:18:40.886129Z","shell.execute_reply.started":"2023-01-02T08:18:40.658545Z"},"trusted":true},"outputs":[{"data":{"text/plain":["CCNN(\n","  (anchor_generator): AnchorGenerator()\n","  (corner_proposal): CornerProposal()\n","  (predictor): ResNet(\n","    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (4): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (5): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): TwoMLPHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (loc_fc): Linear(in_features=512, out_features=4, bias=True)\n","      (cls_fc): Linear(in_features=512, out_features=1, bias=True)\n","      (sigmoid): Sigmoid()\n","    )\n","  )\n",")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# constrast adjustment\n","class ContrastAdjustment:\n","    def __init__(self, factor=5):\n","        self.factor = factor\n","    \n","    def __call__(self, images):\n","        return torchvision.transforms.functional.adjust_contrast(images, self.factor)\n","\n","# init model\n","model = CCNN(predictor=resnet, roi_transform=ContrastAdjustment(factor=5))\n","\n","# init optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.NAdam(params, lr=0.001) # nadam\n","#optimizer = torch.optim.RMSprop(params, lr=0.001, weight_decay=0.9, momentum=0.9)\n","#lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95, last_epoch=-1)\n","\n","# apply he initialization\n","def weights_init(module: nn.Module):\n","    if isinstance(module, nn.Conv2d):\n","        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n","        if module.bias is not None:\n","            nn.init.constant_(module.bias, 0)\n","    elif isinstance(module, nn.BatchNorm2d):\n","        nn.init.constant_(module.weight, 1)\n","        nn.init.constant_(module.bias, 0)\n","    elif isinstance(module, nn.Linear):\n","        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n","        nn.init.constant_(module.bias, 0)\n","\n","model.apply(weights_init)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T09:22:06.590505Z","iopub.status.busy":"2023-01-02T09:22:06.590133Z","iopub.status.idle":"2023-01-02T09:22:06.596598Z","shell.execute_reply":"2023-01-02T09:22:06.595379Z","shell.execute_reply.started":"2023-01-02T09:22:06.590473Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["# setup tensorboard\n","import os\n","PRJ_ROOT = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n","import time\n","from torch.utils.tensorboard import SummaryWriter\n","\n","root_logdir = \"/kaggle/working/runs\"\n","run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n","writer = SummaryWriter(os.path.join(root_logdir, run_id))\n","\n","# compile model with the trainer\n","ccnn_compiled = Trainer(\n","    model, optimizer, nudtsirst_train,\n","    batch_size=32, \n","    valid_ratio=0.003,\n","    valid_freq=0.3, # valid per epoch\n","    tensorboard=writer,\n","    output=os.path.join(PRJ_ROOT, \"logs\", \"checkpoints\", \"ccnn_v1248_0201.pth\"),\n","    input=os.path.join(PRJ_ROOT, \"logs\", \"checkpoints\", \"ccnn_v0431_0201.pth\"),\n","    metric=SIRSTMetrics()\n",")"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T09:36:25.163858Z","iopub.status.busy":"2023-01-02T09:36:25.163482Z","iopub.status.idle":"2023-01-02T09:36:25.172459Z","shell.execute_reply":"2023-01-02T09:36:25.171418Z","shell.execute_reply.started":"2023-01-02T09:36:25.163826Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1180/1180 [100%] - eta: 12.49s - train_loss: 1.6100 - valid_metrics: 0.0298\n","Epoch 2/20\n","1180/1180 [100%] - eta: 11.50s - train_loss: 1.2245 - valid_metrics: 0.1100\n","Epoch 3/20\n","1180/1180 [100%] - eta: 11.24s - train_loss: 1.1001 - valid_metrics: 0.1850\n","Epoch 4/20\n","1180/1180 [100%] - eta: 11.71s - train_loss: 0.0812 - valid_metrics: 0.2434\n","Epoch 5/20\n","1180/1180 [100%] - eta: 10.46s - train_loss: 0.0457 - valid_metrics: 0.2982\n","Epoch 6/20\n","1180/1180 [100%] - eta: 10.59s - train_loss: 0.0113 - valid_metrics: 0.3074\n","Epoch 7/20\n","1180/1180 [100%] - eta: 12.49s - train_loss: 0.0087 - valid_metrics: 0.3125\n","Epoch 8/20\n","1180/1180 [100%] - eta: 13.11s - train_loss: 0.0052 - valid_metrics: 0.3395\n","Epoch 9/20\n","1180/1180 [100%] - eta: 13.02s - train_loss: 0.0041 - valid_metrics: 0.3381\n","Epoch 10/20\n","1180/1180 [100%] - eta: 14.01s - train_loss: 0.0032 - valid_metrics: 0.3265\n","Epoch 11/20\n","1180/1180 [100%] - eta: 09.50s - train_loss: 0.0035 - valid_metrics: 0.3149\n","Epoch 12/20\n","1180/1180 [100%] - eta: 11.32s - train_loss: 0.0026 - valid_metrics: 0.3400\n","Epoch 13/20\n","1180/1180 [100%] - eta: 11.19s - train_loss: 0.0029 - valid_metrics: 0.3201\n","Epoch 14/20\n","1180/1180 [100%] - eta: 10.68s - train_loss: 0.0022 - valid_metrics: 0.3346\n","Epoch 15/20\n","1180/1180 [100%] - eta: 10.41s - train_loss: 0.0021 - valid_metrics: 0.3279\n","Epoch 16/20\n","1180/1180 [100%] - eta: 12.11s - train_loss: 0.0019 - valid_metrics: 0.3282\n","Epoch 17/20\n","1180/1180 [100%] - eta: 12.23s - train_loss: 0.0023 - valid_metrics: 0.3188\n","Epoch 18/20\n","1180/1180 [100%] - eta: 12.67s - train_loss: 0.0026 - valid_metrics: 0.3181\n","Epoch 19/20\n","1180/1180 [100%] - eta: 11.91s - train_loss: 0.0017 - valid_metrics: 0.3182\n","Epoch 20/20\n","1180/1180 [100%] - eta: 10.69s - train_loss: 0.0018 - valid_metrics: 0.3005\n"]}],"source":["# training the model\n","ccnn_compiled.MAX_LOAD_SIZE = 32\n","ccnn_compiled.fit() # 20 epochs"]},{"cell_type":"markdown","metadata":{},"source":["# **5 - Testing Phase**"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T09:11:28.407180Z","iopub.status.busy":"2023-01-02T09:11:28.406712Z","iopub.status.idle":"2023-01-02T09:11:28.412997Z","shell.execute_reply":"2023-01-02T09:11:28.411999Z","shell.execute_reply.started":"2023-01-02T09:11:28.407142Z"},"trusted":true},"outputs":[],"source":["# setup test dataloader\n","data_loader_test = DataLoader(\n","    nudtsirst_test, batch_size=32, \n","    shuffle=False, collate_fn=lambda batch: tuple(zip(*batch)))"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-01-02T09:38:45.814809Z","iopub.status.busy":"2023-01-02T09:38:45.813706Z","iopub.status.idle":"2023-01-02T09:38:45.824545Z","shell.execute_reply":"2023-01-02T09:38:45.823442Z","shell.execute_reply.started":"2023-01-02T09:38:45.814755Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["({'detection_rate_0.0': tensor(0.8130), 'detection_rate_0.5': tensor(0.3341), 'detection_rate_1.0': tensor(0.0025)}, {'false_alarm_rate_0.0': tensor(0.2271), 'false_alarm_rate_0.5': tensor(0.8120), 'false_alarm_rate_1.0': tensor(0.9999)})\n"]}],"source":["# testing the model\n","ccnn_compiled.MAX_LOAD_SIZE = 32\n","ccnn_compiled.evaluate(data_loader_test)"]}],"metadata":{"kernelspec":{"display_name":"cv_sirst_torch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"06d44a258311c3fa192b87b0478b5db1072d38b433ef5912158a323a9f9848e3"}}},"nbformat":4,"nbformat_minor":4}
